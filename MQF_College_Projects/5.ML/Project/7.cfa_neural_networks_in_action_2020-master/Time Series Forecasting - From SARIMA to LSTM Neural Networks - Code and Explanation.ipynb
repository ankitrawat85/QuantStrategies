{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in Action for Time Series Forecasting\n",
    "\n",
    "### Objective: Understand and implement CNNs and LSTMs by forecasting the Nifty500 returns\n",
    "### Data Source : CEIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install Anaconda from the following link\n",
    "\n",
    "https://www.anaconda.com/distribution/\n",
    "\n",
    "\n",
    "However, before installing Anaconda,do refer to the image attached below and remember to select ‘Add Anaconda to my PATH environment variable’ (ignore any warning against doing the same) while you’re installing Anaconda.\n",
    "\n",
    "<img src=\"conda_installation.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The whole installation process should not take more than 5-10mins \n",
    "\n",
    "##### Installing key python libraries\n",
    "\n",
    "Having installed Anancoda with the aforementioned options selected. Run the following two lines in your command prompt.\n",
    "- You can find the command prompt by typing 'cmd' in your search bar as follows:\n",
    "\n",
    "<img src=\"cmd.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Once the command prompt is open,run the following two lines (starting with pip) in it:\n",
    "\n",
    "- pip install tensorflow==1.15.0\n",
    "\n",
    "- pip install keras==2.3.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Basic Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "#make sure you have the correct versions installed\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check version of tensorflow\n",
    "import random as rn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>EURUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/18 17:04</td>\n",
       "      <td>1.20049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/18 17:05</td>\n",
       "      <td>1.20050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/18 17:06</td>\n",
       "      <td>1.20050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/18 17:07</td>\n",
       "      <td>1.20048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/18 17:08</td>\n",
       "      <td>1.20051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>9/4/18 7:55</td>\n",
       "      <td>1.22817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>9/4/18 7:56</td>\n",
       "      <td>1.22835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>9/4/18 7:57</td>\n",
       "      <td>1.22830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>9/4/18 7:58</td>\n",
       "      <td>1.22846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>9/4/18 7:59</td>\n",
       "      <td>1.22821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99991 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date   EURUSD\n",
       "0      1/1/18 17:04  1.20049\n",
       "1      1/1/18 17:05  1.20050\n",
       "2      1/1/18 17:06  1.20050\n",
       "3      1/1/18 17:07  1.20048\n",
       "4      1/1/18 17:08  1.20051\n",
       "...             ...      ...\n",
       "99986   9/4/18 7:55  1.22817\n",
       "99987   9/4/18 7:56  1.22835\n",
       "99988   9/4/18 7:57  1.22830\n",
       "99989   9/4/18 7:58  1.22846\n",
       "99990   9/4/18 7:59  1.22821\n",
       "\n",
       "[99991 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing the directory to where you have downloaded the data\n",
    "#os.chdir('C:\\\\Users\\\\Cafral\\\\Desktop\\\\CAFRAL\\\\Intro MLTS\\\\cfa_data')\n",
    "\n",
    "#df = pd.read_csv('cfa_nn_data.csv')\n",
    "\n",
    "df = pd.read_csv('price.csv')\n",
    "df_1 = df[[\"Date\",\"EURUSD\"]]\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['Unnamed: 0'],inplace=True,axis=1) #removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitrawat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#reformatting and sorting by date\n",
    "df_1['Date'] = pd.to_datetime(df_1['Date']) #always format the date\n",
    "df_1 = df_1.sort_values(['Date'])\n",
    "df_1['Date'] = pd.to_datetime(df_1['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Next, it is important to drop weekends as the Indian stock market does not normally trade on weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing weekends\n",
    "df_1 = df_1[(df_1['Date'].dt.dayofweek != 5)&(df_1['Date'].dt.dayofweek != 6)]\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Now, dealing with missing values: first , all the columns with more that 10% of missing values are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping if ptage of missing values is greater than 10%\n",
    "df_1 = df_1.loc[:, df_1.isna().sum()/df_1.shape[0] <= 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.dropna(axis=0,how='all',subset=df_1.columns[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Second, for the remaining columns with at most 10% missing observation, the missing values are fillied using linear interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear interpolation to deal with missing data\n",
    "date = df_1['Date']\n",
    "df_1 =df_1[[\"Date\",\"EURUSD\"]]\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[df_1.columns[1:]].astype(float).interpolate(method ='linear',axis = 0,limit=30,\n",
    "                                                              limit_direction ='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by date\n",
    "df_1['Date'] = date\n",
    "df_1 = df_1.sort_values(['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Making the Data Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  What is stationarity? \n",
    "- Stationarity is a property of time series data stating that the distributional properties (mean and standard deviation) of the data series has not changed across time. \n",
    "\n",
    "\n",
    "#### 3.2  Why is it important to have stationarity data? \n",
    "- For forecasting it is important that the data be stationary because in the absence of stationarity, one is asking the model to predict data that is nothing like anything it has seen before.\n",
    "\n",
    "#### 3.4 How do you test for stationarity?\n",
    "- A common test of stationarity is the Dickey-Fuller Test. \n",
    "- If the p-value associated with the Dickey-Fuller Test statistic is greater than 0.05, we state that the data is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to test stationarity\n",
    "#SOURCE:https://github.com/tklouie/PyData_LA_2018/blob/master/PyData_LA_2018_Tutorial.ipynb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.stattools\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.tsa.x13\n",
    "from statsmodels.tsa.x13 import x13_arima_select_order, _find_x12\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import statsmodels.graphics.tsaplots as tsaplots\n",
    "\n",
    "def test_stationarity(df, ts):\n",
    "\n",
    "    # Determing rolling statistics\n",
    "    rolmean = df[ts].rolling(window = 12, center = False).mean()\n",
    "    rolstd = df[ts].rolling(window = 12, center = False).std()\n",
    "\n",
    "    # Plot rolling statistics:\n",
    "    orig = plt.plot(df[\"Date\"],df[ts], color = 'blue',label = 'Original')\n",
    "    mean = plt.plot(df[\"Date\"],rolmean, color = 'red',label = 'Rolling Mean')\n",
    "    std = plt.plot(df[\"Date\"],rolstd, color = 'black', label = 'Rolling Std')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title('Rolling Mean & Standard Deviation for %s' %(ts))\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show(block = False)\n",
    "    plt.close()\n",
    "\n",
    "    # Perform Dickey-Fuller test:\n",
    "    # Null Hypothesis (H_0): time series is not stationary\n",
    "    # Alternate Hypothesis (H_1): time series is stationary\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = statsmodels.tsa.stattools.adfuller(df[ts], autolag='AIC') #add kpss\n",
    "    \n",
    "    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistic','p-value', '# Lags Used','Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "    \n",
    "#Applying the function to our Y variable: National Stock Exchange: Index: Nifty 500\n",
    "test_stationarity(df_1, 'EURUSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above results, we see that the p-value is  0.964552, leading to the conclusion that the Nifty500 Index series is not staionary. \n",
    "- It can be futher seen more clearly from the graph - the clear upward trend of the rolling mean and rolling standard deviation signals that Nifty500 is not stationary.\n",
    "\n",
    "### 3.5  So what should one do when the data is not stationary? Make the data as stationary as possible\n",
    "\n",
    "The easiest was of making the data stationary is to claculate the first difference:\n",
    "- For level variables (like indices), the first difference in the percentage change vis-a-vis the previous time period\n",
    "- For percentage variables, its the value of the variable in the current time period subtracted from the value of the variables at the previous date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to calculate percentage change\n",
    "def percentChange(x,numLags):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    x: Column for which we want to calculate percent change\n",
    "    numLags: The number of days from when the change needs to be calculated. \n",
    "            Example : If using daily data - numLags = 1 for daily change\n",
    "                                            numLags = 30 for monthly change\n",
    "                                            numLags = 365 for yearly change       \n",
    "    OUTPUT:\n",
    "    percentage change in variable\n",
    "    '''\n",
    "    y = (x - x.shift(numLags))/x.shift(numLags)\n",
    "    return y\n",
    "\n",
    "dataForMl = pd.DataFrame()\n",
    "dataForMl['Date'] = df_1['Date']\n",
    "\n",
    "#here, I only have level variables so I do not need separate my variables into level vs non-level variables\n",
    "levelVars = df_1.columns[:-1]\n",
    "for levelVar in levelVars:\n",
    "    dataForMl[f'{levelVar}Ret'] = percentChange(df[levelVar],1)\n",
    "    \n",
    "dataForMl = dataForMl[1:] #ignoring the first row as it contains null values\n",
    "dataForMl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the percentage change for all our variables, let's check if our data is stationary now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(dataForMl, 'EURUSDRet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above graphs, we see that the percentage change in Nifty 500 Index (i.e. National Stock Exchange: Index: Nifty 500Ret) is stationary as the p-value is less that 0.05. \n",
    "- Again, for greater intuition look at the graph - the rolling mean and rolling standard deviation lines are along are almost flat meaning that they don't change in a statistically significant manner over time.\n",
    "- However, do note that taking the first difference does not necessarily have to make the data stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lagging the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, because sequential/time series data is auto regressive in nature - i.e. the outcome today depends on the outcome yesterday and the outcome the day before yesterday and so on - we need to create lagged versions of each independent ariable. \n",
    "\n",
    "#### So how many lags of the independent variables do we want to include?\n",
    "\n",
    "- The maximum number of lags of the dependent variable to use can be decided from the Autocorrelation Function (ACF) and/or Partial Autocorrelation Function (PACF) plot or decided huristically based on domain specific cycles (ex:a business cycle,seasons etc). \n",
    "> ##### Autocorrelation function (ACF). At lag k, this is the correlation between series values that are k intervals apart.\n",
    "> ##### Partial autocorrelation function (PACF). At lag k, this is the correlation between series values that are k intervals apart, accounting for the values of the intervals between.\n",
    "\n",
    "- The maximum number of lags of other independent variables to include is rather arbitrary -  can be decided huristically based on domain specific cycles (ex:a business cycle,seasons etc). \n",
    "\n",
    "- The minimum number of lags to include depending on the forcasting horizon. If you want to forecast h steps ahead, you exclude the first h lags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make the acf and pacf plots using the statsmodels library in Python\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import statsmodels.graphics.tsaplots as tsaplots\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "tsaplots.plot_acf(dataForMl['EURUSDRet'].astype(float),lags =25)\n",
    "tsaplots.plot_pacf(dataForMl['EURUSDRet'].astype(float),lags =25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both the ACF and PACF plots show high serial correlation at the first lag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since we're going to forecast the one day ahead Nifty stock returns, the minimum lag considered by me is 1\n",
    "minLagNum = 1\n",
    "\n",
    "#lagging the vars :here i'm iginoring the ACF and PACF lag structure and deciding the maximum number of lags heuristically\n",
    "maxLagNum = 10 #here I have chosen the maxLagNum arbitrarily. A better strategy is to look at the acf plot\n",
    "dataForMl = dataForMl.sort_values(['Date'])\n",
    "for column in dataForMl.set_index(\"Date\").columns:\n",
    "    for lag in range(minLagNum,maxLagNum+1):\n",
    "        dataForMl[f'{column}Lag_{lag}'] = dataForMl[f'{column}'].shift(lag)\n",
    "        \n",
    "\n",
    "dataForMl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by date\n",
    "dataForMl = dataForMl.sort_values(['Date'])\n",
    "\n",
    "#removing columns if nan value in a column\n",
    "dataForMl = dataForMl.dropna()\n",
    "\n",
    "#specifying independent variables:including only lagged versions of variables and excluding date variables\n",
    "final_vars = [col for col in dataForMl.columns if (col.find('Lag')!=-1) & (col.find('date')==-1)]\n",
    "\n",
    "#specifying the dependent variable\n",
    "dep_var = 'EURUSDRet'\n",
    "\n",
    "#always make the dependent ariable the last column in the dataset\n",
    "final_vars.append(dep_var)\n",
    "\n",
    "#for later use\n",
    "dataForMl_copy = dataForMl\n",
    "\n",
    "#keeping only relevant \n",
    "dataForMl = dataForMl[final_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training, Validation and Testing\n",
    "\n",
    "- The objective of a machine learning methods is to predict the dependent variable as accurately as possible i.e. to minimize the predicted error (the difference between the actual value and predicted value).\n",
    "\n",
    "- To measure the predictive accuracy of a model, it is important that the forecast accuracy be measured out-of sample as the training accuracy can be made arbitrarily high through overfitting. \n",
    "- However, if we use the entire out-of-sample data for testing, we may overfit to the out-of-sample data (a phenomenon known as ‘data leakage’), resulting poor true generalizability. \n",
    "- To protect against ‘data leakage’, we split the out-of-sample data into two parts: validation data and testing data. \n",
    "\n",
    "- The validation  set allows the evaluation of the model on unseen data to select the best model architecture, while still holding out a subset of data for final evaluation after finding the best model. \n",
    "\n",
    "### 5.1 The training, validation and testing data can be organized in many ways, namely:\n",
    "- cross validation (bootstrap sampling for cross sectional methods) : not suited for time series\n",
    "- fixed window (training, validation and testing periods demarcated by dates)\n",
    "- rolling window (shifting a window of fixed size ahead by one observation successively) / expanding window (increasing the window size by 1 successively). \n",
    "\n",
    "Given the time constraint, here I choosed a fixed window approach. The rolling or expanding window approach is highly reccomended if the time is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking the data into train and test along time dim\n",
    "test_percent = 0.10\n",
    "no_test_obs =  int(np.round(test_percent*len(dataForMl)))\n",
    "training = dataForMl[:-no_test_obs]\n",
    "testing = dataForMl[-no_test_obs:]\n",
    "\n",
    "#breaking the testing data into validation and out of sample data\n",
    "validation_percent = 0.70\n",
    "no_validation_obs = int(np.round(validation_percent*len(testing)))\n",
    "validation = testing[:-no_validation_obs]\n",
    "outOfSample = testing[-no_validation_obs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Rescaling Normalized Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normailze the data in the training sample because machine learning methods are not scale invariant\n",
    "- Then we scale it to being between -1 and 1 as that is the appropriate scaling for data when being input into a Convolutional Neural Network (CNN) and Long Short Term Memory Network(LSTM). \n",
    "- Here this scaling for all the methods to make the prediction errors comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "trainMinmax = min_max_scaler.fit_transform(training.values) #fit and transform training data\n",
    "valMinmax = min_max_scaler.transform(validation.values)\n",
    "outSampleMinmax = min_max_scaler.transform(outOfSample.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the rescaling parameters from the training sample are used to rescale the data in the validation and testing samples to ensure that the out-of-sample data does not have a look forward bias or 'data leakage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking the data into independent variables (x) and dependent variables (y)\n",
    "\n",
    "#training independent, dependent\n",
    "trainMinmax_x,trainMinmax_y = trainMinmax[:,:-1],trainMinmax[:,-1] \n",
    "\n",
    "#validation independent, dependent\n",
    "valMinmax_x,valMinmax_y = valMinmax[:,:-1],valMinmax[:,-1]\n",
    "\n",
    "#out of sample testing independent, dependent\n",
    "outSampleMinmax_x,outSampleMinmax_y = outSampleMinmax[:,:-1],outSampleMinmax[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.  Metric to be used to guage accuracy of forecasts\n",
    "- The most commonly used metric for guaging the accuracy of time series forecasts is the Mean Squared Error (MSE)\n",
    "- The higher the MSE, the worse is the predictive accuracy of the model.\n",
    "- Thus for each model,it is desired that the MSE be minimized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Cafral\\\\Desktop\\\\CAFRAL\\\\Intro MLTS\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 So why is Linear Regression (OLS) not best suited for forecasting Time Series Data?\n",
    "\n",
    "<img src=\"ols_short_comings.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "###  8. 2 Potential Solution : Seasonal Autoregressive Integrated Moving Average (SARIMA)\n",
    "\n",
    "### 8.2.1 What is SARIMA?\n",
    "- ARIMA models describe how each successive observation is related to the previous observation\n",
    "- The seasonal ARIMA (SARIMA) is capable of modelling the seasonal components in a univariate time series in addition to the autoregressive, moving average and trend components typically modelled by ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING SARIMA MODEL\n",
    "import statsmodels.tsa.stattools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#(p,d,q) are determined using  Autocorrelation Function (ACF) , Partial Autocorrelation Functions (PACF) and tests for stationary.  \n",
    "#How do we interpret ACF and PACF plots?\n",
    "\n",
    "#p – Lag value where the PACF chart crosses the upper confidence interval for the first time.\n",
    "#q – Lag value where the ACF chart crosses the upper confidence interval for the first time.\n",
    "\n",
    "p= 1 #the trend autoregressive order.\n",
    "d= 0 #the trend difference order.\n",
    "q= 1 #the trend moving average order\n",
    "\n",
    "P= 1 #the number of seasonal autoregressive terms.\n",
    "D= 0 #the number of seasonal difference terms\n",
    "Q= 1 #the number of seasonal moving average terms\n",
    "M=1 #the number of time steps for a seasonal period \n",
    "\n",
    "myorder = (p,d,q)\n",
    "myseasonalorder = (P,D,Q,M)\n",
    "model = sm.tsa.statespace.SARIMAX(trainMinmax_y,\n",
    "                                  order=myorder, \n",
    "                                  seasonal_order=myseasonalorder,\n",
    "                                  trend='c')#,exog=trainMinmax_x \n",
    "\n",
    "#Training the model\n",
    "model_fit = model.fit() \n",
    "\n",
    "# In case you want to add exgogenous variables\n",
    "#exogNormal = normalizer.transform(testing.values)\n",
    "#exogMinmax = min_max_scaler.transform(exogNormal)\n",
    "#exogMinmax_x,exogMinmax_y = exogMinmax[:,:-1],exogMinmax[:,-1]\n",
    "\n",
    "#PredictingHi \n",
    "totat_preds = len(testing)#len(exogMinmax_y)\n",
    "sarimaPred = model_fit.predict(start=1, end=totat_preds)#exog=exogMinmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(actual_y,predicted_y,method,date):\n",
    "    mse = mean_squared_error(actual_y,predicted_y)\n",
    "    \n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(date,actual_y)\n",
    "    plt.plot(date,predicted_y)\n",
    "    plt.legend(['Actual','Predicted'])\n",
    "    plt.title(f'{method} (MSE: {mse})')        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(valMinmax_y,sarimaPred[:len(valMinmax_y)],'SARIMA Validation',range(len(valMinmax_y)))\n",
    "plot_results(outSampleMinmax_y,sarimaPred[len(valMinmax_y):],'SARIMA Testing',range(len(outSampleMinmax_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sarima_vs_ols.PNG==100x20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 SARIMA vs. OLS for Time Series Data\n",
    "<img src=\"sarima_vs_ols.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "### 8.2.3 Additional disadvantages of SARIMA : challenges of ARIMA models\n",
    "- Potential for error propagation\n",
    "- Backward looking: “they are generally poor at predicting turning points (unless the turning point represents a return to a long-run equilibrium)”.\n",
    "- Mean-reverting: As the number of steps forecasted ahead in the future increases , the forecast converges to the mean\n",
    "> But this is not a problem limited to ARIMA type models\n",
    "\n",
    "### 8.3 Potential Solution : Deep Neural Networks (DNNs)\n",
    "\n",
    "#### 8.3.1 Why Only Focus on Deep Neural Networks?\n",
    "<img src=\"why_only_nn.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### 8.3.2 What is a Neural Network?\n",
    "\n",
    "- In general, neural networks are composite functions which are universal function approximators.\n",
    "\n",
    "- Every neural network is broadly composed of three types of layers: \n",
    "\n",
    "  a. the input layer\n",
    "  \n",
    "  b. the hidden layer / the computational layer\n",
    "  \n",
    "  c. an output layer\n",
    "\n",
    "- Each of the 3 layers is comprised of multiple nodes and is connected to the subsequent layer through weights\n",
    "\n",
    "For some intution, let's model a linear regression with 3 independent variable as a neural network:\n",
    "<img src=\"lin_reg_as_nn.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "#### 8.3.3 What is a deep neural network?\n",
    "- Neural Network with Multiple Hidden Layers\n",
    "\n",
    "#### 8.3.4 Where does the ‘learning’ in a neural network come from?\n",
    "\n",
    "<img src=\"dnn_learn_ffn.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "#### 8.3.5 But Not All Neural Networks Are Suited for Time Series…\n",
    "- Look at linear regression as a neural network : \n",
    "\n",
    "    1) firstly,it treats the data cross sectionally. What does that mean?\n",
    "    \n",
    "    2) no new features are created : does not find non-linearities in the data \n",
    "        Solution: Use a deep learning network -> creates non-linearities -> but problem: still treats data cross sectionally\n",
    "- Thus, a feed forward neural network cannot guage information from the spatial and sequential nature of time series data \n",
    "- Thus, it is not recommended that a fully connected feed forward neural network because it is dominated by more complex deep neural networks that can infer information from time series data\n",
    "- Here we consider two methods:\n",
    "\n",
    "    A) 1 Dimensional Convolutional Neural Networks (1D-CNNs)\n",
    "  \n",
    "    B) Long Short Term Memory (LSTM) Networks\n",
    "    \n",
    "#### 8.3.6 What makes CNNs and LSTMs different?\n",
    "- OLS and SARIMA depend on hand-selected independent variables supplied manually\n",
    "  - They do not use these independent variables to create new independent variables or 'features'\n",
    "- However, both CNNs and LSTMs can “learn an internal representation of the time series data and ideally achieve comparable performance to models fit on a version of the dataset with engineered features” \n",
    "    How?\n",
    "    - The input layer preserves the temporal structure of data\n",
    "    - The hidden layer/s use complex computational nodes: The different computational nodes used in a CNN vis-à-vis a LSTM network result in the two  type of networks processing the data differently.\n",
    "    \n",
    "#### 8.3.7 What does it mean to preserve the temporal structure of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "#split a multivariate sequence into samples that preserve the temporal structure of the data\n",
    "#SOURCE:https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in =30 #number of observations from the past that we assume to be relevant across time for forecasting\n",
    "n_steps_out = 1 #number of units ahead that we want to forecast into the future\n",
    "\n",
    "#training sequence\n",
    "trainSeq_x, trainSeq_y = split_sequences(trainMinmax, n_steps_in,n_steps_out)\n",
    "\n",
    "#out of sample sequence\n",
    "validationSeq_x, validationSeq_y= split_sequences(valMinmax, n_steps_in,n_steps_out)\n",
    "\n",
    "#out of sample sequence\n",
    "outSampleSeq_x, outSampleSeq_y= split_sequences(outSampleMinmax, n_steps_in,n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMinmax.shape #Output: (rows,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSeq_x.shape #Output: (number of samples,size of 'window' /timesteps,number of independent variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "\n",
    "- The intitial weights of a neural network at assigned randomly and greatly affect how the network performs. \n",
    "- If you run your neural network - with the exactly same code and hyper-parameters - repeately, you will get different results\n",
    " - Solution: Fix the random weights to the same weights every time the neural network is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Convolutional Neural Network\n",
    "\n",
    "- Convolutional neural networks (CNNs) can extract information from the temporal structure of the data by:\n",
    "       a) preserving the spatial/ temporal structure of the data in the input layer\n",
    "       b) using filters which look for patterns in spatially adjacent data\n",
    "       - For instance, one filter could finding pseaks, another could find troughs while another could find a linear trend\n",
    "       - The information extracted by the filters is known as a feature map\n",
    "       - Each additional feature results in a new feature map extracting a more complex feature\n",
    "       - For a time series dataset, a filter can only move along 1 dimension - time\n",
    "\n",
    "- The CNN layer may be followed by a sub-sampling layer which reduces the noise in the learned features (i.e. the feature maps)\n",
    "\n",
    "- The sub-sampling layer is followed by a regression layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a neural network in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, AveragePooling1D,MaxPooling1D\n",
    "from keras.layers import Conv1D,AveragePooling1D,MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1,l2,l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# For Replicability : Always run this as one cell ##########################################\n",
    "#SOURCE :\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "#tf.random.set_random_seed(1234)\n",
    "#from keras import backend as K\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "###########################################################################################################################\n",
    "\n",
    "#While training the neural network, it is important that we use the MSE error of the validation set to decide when to \n",
    "#stop training our network. If we use the MSE of the training set, we will not get good predictions in the test set due \n",
    "#to over fitting. However, unlike the error in the training set, the error in the validation set does not reduce with \n",
    "#every passing epoch. Sometimes, it increases for a while before it starts declining. The patience argument in Earlystop allows us \n",
    "#to decide how many times we want the validation error to keep increasing before we stop training the neural network.\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto',restore_best_weights=True)\n",
    "\n",
    "epochs = 100000 #number of times the feed forward mechanism and back propagation are repeated \n",
    "\n",
    "bs = 100 #batch size for SGD :show what happens when batch size very small\n",
    "lr =0 #learning rate: the degree to which the weights are updated by each batch of SGD\n",
    "\n",
    "sgd = SGD(lr=lr) #type of optimizer - Alternative: ADAM, NADAM\n",
    "\n",
    "X, y = split_sequences(trainMinmax, n_steps_in,n_steps_out)\n",
    "n_features = X.shape[2]\n",
    "\n",
    "#np.random.seed(0)\n",
    "\n",
    "model = Sequential() #initializing keras Sequential model\n",
    "\n",
    "#convolutional layer starts\n",
    "model.add(Conv1D(filters=5,#number of filters\n",
    "                 kernel_size=2,#size of the filte racross time\n",
    "                 strides=2, #number of rows that the filter moves ahead by\n",
    "                 activation='linear',#transformation\n",
    "                 input_shape=(n_steps_in, n_features))) #shape of 1 sample : preserves temporal structure\n",
    "                #kernel_regularizer=l2(0.009),, bias_regularizer=l2(0.01)\n",
    "#convolutional layer ends\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=1)) #sub-sampling layer - Alternative: AveragePooling1D\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1)) #reduces overfitting by dropping some weights randomly\n",
    "\n",
    "#regression layer begins\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,#number of outputs\n",
    "                activation='tanh',#transformation:other options include - ReLU,Linear,Sigmoid\n",
    "                kernel_regularizer=l2(0.01))) \n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd') #specifies which optimizer and loss funtion to useto use \n",
    "\n",
    "#training the model\n",
    "model.fit(trainSeq_x, trainSeq_y,\n",
    "          batch_size=bs,\n",
    "          epochs=epochs, \n",
    "          callbacks= [EarlyStop],\n",
    "          verbose=2, \n",
    "          shuffle=False,#always set to false for time series data\n",
    "          validation_data =(validationSeq_x, validationSeq_y))\n",
    "                         \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation metrics \n",
    "cnnValPred = model.predict(validationSeq_x)\n",
    "\n",
    "#testing prediction\n",
    "cnnOutSamplePred = model.predict(outSampleSeq_x)\n",
    "    \n",
    "plot_results(validationSeq_y ,cnnValPred  ,'CNN Validation',range(len(validationSeq_y)))\n",
    "plot_results(outSampleSeq_y ,cnnOutSamplePred  ,'CNN Testing',range(len(outSampleSeq_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Long Short Term Memory Network (LSTM)\n",
    "- A key shortcoming of CNNs for time series data is that they do not draw information from the sequential nature of independent data.\n",
    "- So, how does one build a neural network that can remember and extract information from a sequence? Long Short Term Memory (LSTM) Network.\n",
    "    - LSTM networks draw information from the sequential nature of the data because they have memory i.e. it remembers what it has seen\n",
    "    - LSTMs belong to a larger class of neural networks known as Recurrent Neural Networks, all of which have memory.\n",
    "    - Of many RNNs , LSTM are chosen here over a vanilla RNN as the former can learn from long sequences while the latter may not.\n",
    "\n",
    "#### 8.5.1 How does an LSTM network build short and long term memory? \n",
    "LSTMs build short and long term memories by revealing data to the hidden nodes in a sequential fashion.\n",
    "\n",
    "#### 8.5.2 How does sequential revealing allow an LSTM Node to Build Memory?\n",
    "\n",
    "Each LSTM node is comprised of LSTM cells. \n",
    "\n",
    "The long term and short term memory is updated in each LSTM cell upon being exposed to each subsequent element of the sequence , conditional on the output of each ‘gate’.\n",
    "\n",
    "#### 8.5.3 What is a gate?\n",
    "\n",
    "- Each “gate” is a neural network with a sigmoid/logistic activation function which determines how the memory updated by accepting as inputs:\n",
    "    a) the current elements X of the sequence\n",
    "\n",
    "    b) the outputs of the previous LSTM cell i.e. the long term memory and hidden memory from the previous member of the sequece\n",
    "\n",
    "- Each gate outputs a number between 0 (meaning no information is transferred ) and 1 (meaning all information is transferred).\n",
    "\n",
    "- In each LSTM cell, the following gates are used in the following order to build and update long term memory and short term memory:\n",
    "    - The Forget Gate / Remember Vector\n",
    "    - Candidate Gate\n",
    "    - Input Gate / Save Vector\n",
    "    - Output gate / Focus Vector\n",
    "\n",
    "- The updated working memory at the end of the sequence is the output of an LSTM node. \n",
    "- This working memory or hidden memory is the new internal representation of the data that is learnt by an LSTM network i.e. the working memory is the new feature used as an input in the regression layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best\n",
    "from keras.layers import LSTM\n",
    "\n",
    "################################### Set for replicability ##################################################################\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "#tf.random.set_random_seed(1234)\n",
    "#from keras import backend as K\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "###########################################################################################################################\n",
    "\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto',restore_best_weights=True)\n",
    "epochs = 100000\n",
    "lr =0\n",
    "\n",
    "sgd = SGD(lr=lr) #; adam = Adam(lr=lr) ;nadam = Nadam(lr=lr)\n",
    "bs = 100\n",
    "\n",
    "n_steps_in =30\n",
    "n_steps_out = 1\n",
    "\n",
    "#training sequence\n",
    "trainSeq_x, trainSeq_y = split_sequences(trainMinmax, n_steps_in,n_steps_out)\n",
    "\n",
    "#out of sample sequence\n",
    "validationSeq_x, validationSeq_y= split_sequences(valMinmax, n_steps_in,n_steps_out)\n",
    "\n",
    "#out of sample sequence\n",
    "outSampleSeq_x, outSampleSeq_y= split_sequences(outSampleMinmax, n_steps_in,n_steps_out)\n",
    "\n",
    "X_useless, y_useless = split_sequences(trainMinmax, n_steps_in,n_steps_out)\n",
    "n_features = X_useless.shape[2]\n",
    "\n",
    "np.random.seed(0); print(np.random.rand(4))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(300, #number of LSTM nodes\n",
    "               input_shape=(n_steps_in, n_features),\n",
    "               activation = 'tanh')) #ransformation:best to not use any other type of transformation\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1,activation = 'linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "#model\n",
    "model.fit(trainSeq_x, trainSeq_y,batch_size=bs,epochs=epochs, callbacks= [EarlyStop] ,verbose=2, shuffle=False,\n",
    "                         validation_data =(validationSeq_x, validationSeq_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation metrics \n",
    "lstmValPred = model.predict(validationSeq_x)\n",
    "#out of sample metrics\n",
    "lstmOutSamplePred = model.predict(outSampleSeq_x)\n",
    "\n",
    "plot_results(validationSeq_y ,lstmValPred  ,'LSTM Validation',range(len(validationSeq_y )))\n",
    "plot_results(outSampleSeq_y ,lstmOutSamplePred  ,'LSTM Testing',range(len(outSampleSeq_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Saving a Neural Network Model to use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for storing output\n",
    "dirName = f'C:\\\\Users\\\\Cafral\\\\Desktop\\\\CAFRAL\\\\Intro MLTS\\\\output' #change name to suitable folder here\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")\n",
    "\n",
    "####################################### change here onwards #######################################################\n",
    "\n",
    "#change directory\n",
    "os.chdir(dirName)\n",
    "\n",
    "#specify model name\n",
    "model_name='lstm'\n",
    "filename = f'{model_name}_model.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Calling a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "#change directory\n",
    "model_name='lstm' #change name of your model here\n",
    "filename = f'{model_name}_model.h5' \n",
    "saved_model = keras.models.load_model(filename)\n",
    "\n",
    "\n",
    "yhat = saved_model.predict(outSampleSeq_x) \n",
    "mse = mean_squared_error(outSampleSeq_y,cnnOutSamplePred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Invert the Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "def invert_scaling(x_minmax,y_predicted,n_steps_in):\n",
    "    reshaped_pred = y_predicted.reshape((y_predicted.shape[0],1))\n",
    "    matrix = concatenate((x_minmax[n_steps_in-1:],reshaped_pred), axis=1)\n",
    "    matrix_min_max_invert = min_max_scaler.inverse_transform(matrix)\n",
    "    inv_pred = matrix_min_max_invert[:,-1]\n",
    "    return inv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_yhat = invert_scaling(outSampleMinmax_x,cnnOutSamplePred,n_steps_in)\n",
    "plot_results(dataForMl_copy['National Stock Exchange: Index: Nifty 500Ret'][-(no_validation_obs-n_steps_in+1):].values,\n",
    "             inv_yhat,\n",
    "             'CNN Original Values',\n",
    "             dataForMl_copy['date'][-(no_validation_obs-n_steps_in+1):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "<img src=\"summary_full.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"hyperpara.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "# Important Caveats\n",
    "- Machine learning methods, especially DNNs work best with large amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Tibshirani and Friedman (2017)\n",
    "\n",
    "Hastie, James , Tibshirani and Witten (n.d.)\n",
    "\n",
    "Goodfellow , Bengio and Courville (2016)\n",
    "\n",
    "https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/\n",
    "    \n",
    "https://github.com/tklouie/PyData_LA_2018/blob/master/PyData_LA_2018_Tutorial.ipynb\n",
    "    \n",
    "https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model\n",
    "\n",
    "https://stackoverflow.com/questions/38080035/how-to-calculate-the-number-of-parameters-of-an-lstm-network\n",
    "\n",
    "https://stackoverflow.com/questions/50947079/keras-understanding-the-number-of-trainable-lstm-parameters\n",
    "\n",
    "https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model/27067#comment39272_27067\n",
    "\n",
    "https://www.coursera.org/lecture/ai/number-of-parameters-A3N10\n",
    "\n",
    "https://blog.acolyer.org/2019/02/25/understanding-hidden-memories-of-recurrent-neural-networks/\n",
    "\n",
    "https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "\n",
    "http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "\n",
    "https://www.youtube.com/watch?v=lUhtcP2SUsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
