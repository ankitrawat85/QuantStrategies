{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "bikes = pd.read_csv('bike_sharing.csv')\n",
    "num_row = bikes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "train = np.random.choice([True, False], num_row, replace = True, p = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will have several ways to organize our explanatory variables, to better compare these models, let's fix the instances for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bikes['registered'].values\n",
    "y_train, y_test = y[train], y[~train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is 'casual', we split the entire column into training and test sets, according to the True/False flags in the array 'train'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "x1 = bikes[selected_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first construct a very simple regression model from numerical columns as listed above.<br/>\n",
    "In order to compare the coefficients to identify important columns, it is necessary to scale all columns to interval [0, 1] using <i>MinMaxScaler</i>.<br/>\n",
    "The three cells below listed three ways of doing min-max-scaling, please argue which way is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn import preprocessing\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx1 = min_max_scaler.fit_transform(x1)\\nx1_train, x1_test = x1[train,:], x1[~train,:]\\n'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x1 = min_max_scaler.fit_transform(x1)\n",
    "x1_train, x1_test = x1[train,:], x1[~train,:]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn import preprocessing\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx1_train, x1_test = x1[train,:], x1[~train,:]\\nx1_train = min_max_scaler.fit_transform(x1_train)\\nx1_test = min_max_scaler.fit_transform(x1_test)\\n'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x1_train, x1_test = x1[train,:], x1[~train,:]\n",
    "x1_train = min_max_scaler.fit_transform(x1_train)\n",
    "x1_test = min_max_scaler.fit_transform(x1_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x1_train, x1_test = x1[train,:], x1[~train,:]\n",
    "x1_train = min_max_scaler.fit_transform(x1_train)\n",
    "x1_test = min_max_scaler.transform(x1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1-2:\n",
    "The correct way is cell ____________.\n",
    "\n",
    "Third cell is right answer : \n",
    "split data and perform fit_tranform on test data and later perform  transform on train data. \n",
    "\n",
    "Justification : test data and training data should be independent and if we do normalization before splitting data then test data and training data will not be independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.0 \t 0.3337561009257083\n",
      "R2 score: 0.3337561009257083\n",
      "season \t 0 48.552579241088914\n",
      "yr \t 1 67.47645896552172\n",
      "mnth \t 2 0.0\n",
      "hr \t 3 143.8534458356691\n",
      "holiday \t 4 -0.0\n",
      "weekday \t 5 0.0\n",
      "workingday \t 6 36.22018792716402\n",
      "weathersit \t 7 -0.0\n",
      "temp \t 8 141.3067102408608\n",
      "atemp \t 9 0.0\n",
      "hum \t 10 -110.50540041653332\n",
      "windspeed \t 11 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lasso = linear_model.Lasso(alpha = 1.0)\n",
    "lasso.fit(x1_train, y_train)\n",
    "print('alpha:', 1.0, '\\t', lasso.score(x1_test, y_test))\n",
    "print('R2 score:', lasso.score(x1_test, y_test))\n",
    "for i in range(len(selected_cols)):\n",
    "    print(selected_cols[i], '\\t',i,lasso.coef_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ score is 0.33, and there are already 5 columns used for this model (columns with non-zero coefficients).<br/>\n",
    "Let's now decrease alpha to see if we can obtain a model with $R^2$ above 0.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.0\n",
      "R2 score: 0.3337561009257083\n",
      "season \t 48.552579241088914\n",
      "yr \t 67.47645896552172\n",
      "mnth \t 0.0\n",
      "hr \t 143.8534458356691\n",
      "holiday \t -0.0\n",
      "weekday \t 0.0\n",
      "workingday \t 36.22018792716402\n",
      "weathersit \t -0.0\n",
      "temp \t 141.3067102408608\n",
      "atemp \t 0.0\n",
      "hum \t -110.50540041653332\n",
      "windspeed \t 0.0\n",
      "alpha: 0.5\n",
      "R2 score: 0.3372583709988252\n",
      "season \t 51.43172734035656\n",
      "yr \t 68.9565433348504\n",
      "mnth \t 0.0\n",
      "hr \t 146.2126971051315\n",
      "holiday \t -0.0\n",
      "weekday \t 0.0\n",
      "workingday \t 38.30510097122085\n",
      "weathersit \t -0.0\n",
      "temp \t 115.682864388095\n",
      "atemp \t 40.82193204642952\n",
      "hum \t -123.60515918120396\n",
      "windspeed \t 0.0\n",
      "alpha: 0.2\n",
      "R2 score: 0.3397512252932096\n",
      "season \t 53.327034965170434\n",
      "yr \t 69.99539206397505\n",
      "mnth \t 0.0\n",
      "hr \t 147.44979401570717\n",
      "holiday \t -0.0\n",
      "weekday \t 1.6155881949577173\n",
      "workingday \t 39.59279584817247\n",
      "weathersit \t -4.393396287320516\n",
      "temp \t 50.536611161421845\n",
      "atemp \t 124.02925347036496\n",
      "hum \t -127.61529270633936\n",
      "windspeed \t 11.643614632563553\n",
      "alpha: 0.1\n",
      "R2 score: 0.34049016925822095\n",
      "season \t 53.32487940523828\n",
      "yr \t 70.36185840048024\n",
      "mnth \t 0.9928328506782911\n",
      "hr \t 147.79247481689308\n",
      "holiday \t -1.3114906367495487\n",
      "weekday \t 2.480098498543455\n",
      "workingday \t 39.91029699568696\n",
      "weathersit \t -6.213745656254039\n",
      "temp \t 27.05884511668808\n",
      "atemp \t 153.97093277602715\n",
      "hum \t -128.47221598173022\n",
      "windspeed \t 17.362803023359966\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1.0, 0.5, 0.2, 0.1]:\n",
    "    lasso = linear_model.Lasso(alpha = alpha)\n",
    "    lasso.fit(x1_train, y_train)\n",
    "    print('alpha:', alpha)\n",
    "    print('R2 score:', lasso.score(x1_test, y_test))\n",
    "    for i in range(len(selected_cols)):\n",
    "        print(selected_cols[i], '\\t', lasso.coef_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, no.<br/>\n",
    "Now let's consider the column 'weekday'. Although it is a numerical column, the numbers does not have a quantitative meaning, they are just indicators. Such columns are called **Nominal** columns, i.e., for each value x in the column 'weekday', we create a column called 'weekday=x', a record will get a value of 1 on this column if its orginal value is x, otherwise 0.<br/>\n",
    "pandas conviniently provides a function 'get_dummies' for this purpose.<br/>\n",
    "Both columns 'season' and 'weekday' are nominal columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = pd.get_dummies(bikes['season'], prefix = 'season_=')\n",
    "weekdays = pd.get_dummies(bikes['weekday'], prefix = 'weekday_=')\n",
    "\n",
    "bikes = pd.concat([bikes, seasons, weekdays], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting column is column 'weathersit'. The description tells us these four situations are very different.\n",
    "1. Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "2. Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "3. Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "4. Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\n",
    "First, Situation 1 is mutually exclusive from the rest. Second, there is a degree of severity associated from situation 2 to 4, meaning if situation 3 happens, the weather has gone passed situation 2. This is so-called **ordinal** columns.<br/>\n",
    "For ordinal columns, we should turn on lower grade indicator automatically if the value is higher. For example, indicator for weathersit2 should be 1 all records with weathersit 2, 3, 4.<br/>\n",
    "Third, there are very few records for weathersit = 4, that we do not need to create one column just for weathersit 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution : 1-3 \n",
    "Season    :  4 Variables ; Weekday : 7 Variables and  weathersit : 3 variables\n",
    "Number of unique values in a columns determines number of dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['weathersit_=_1'] = 1 * (bikes['weathersit'] == 1)\n",
    "bikes['weathersit_>=_2'] = 1 * (bikes['weathersit'] >= 2)\n",
    "bikes['weathersit_>=_3'] = 1 * (bikes['weathersit'] >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['yr', 'mnth', 'hr', 'holiday', 'workingday', 'temp', 'atemp', 'hum', 'windspeed', 'season_=_1', 'season_=_2', 'season_=_3', 'season_=_4', 'weekday_=_0', 'weekday_=_1', 'weekday_=_2', 'weekday_=_3', 'weekday_=_4', 'weekday_=_5', 'weekday_=_6', 'weathersit_=_1', 'weathersit_>=_2', 'weathersit_>=_3']\n",
    "x2 = bikes[selected_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train, x2_test = x2[train,:], x2[~train,:]\n",
    "x2_train = min_max_scaler.fit_transform(x2_train)\n",
    "x2_test = min_max_scaler.transform(x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.0\n",
      "R2 score: 0.33921775613761235\n",
      "yr \t 67.24905214139518\n",
      "mnth \t 0.0\n",
      "hr \t 144.67172788492135\n",
      "holiday \t -0.0\n",
      "workingday \t 36.18694776794533\n",
      "temp \t 153.22080212218356\n",
      "atemp \t 0.0\n",
      "hum \t -100.8975043131971\n",
      "windspeed \t 0.0\n",
      "season_=_1 \t -22.862338921225902\n",
      "season_=_2 \t 0.0\n",
      "season_=_3 \t -0.0\n",
      "season_=_4 \t 29.9779373760094\n",
      "weekday_=_0 \t -0.36491197541835685\n",
      "weekday_=_1 \t -0.0\n",
      "weekday_=_2 \t 0.0\n",
      "weekday_=_3 \t 0.0\n",
      "weekday_=_4 \t 0.0\n",
      "weekday_=_5 \t -0.0\n",
      "weekday_=_6 \t 0.0\n",
      "weathersit_=_1 \t -0.0\n",
      "weathersit_>=_2 \t 0.0\n",
      "weathersit_>=_3 \t -20.931624064162538\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.Lasso(alpha = 1.0)\n",
    "lasso.fit(x2_train, y_train)\n",
    "print('alpha:', 1.0)\n",
    "print('R2 score:', lasso.score(x2_test, y_test))\n",
    "for i in range(len(selected_cols)):\n",
    "    print(selected_cols[i], '\\t', lasso.coef_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these indicator columns are not as significant as columns, 'yr', 'hr', 'workingday', 'temp' and 'hum'. Even with these five columns only, the performace is about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.0\n",
      "R2 score: 0.3185266344678772\n",
      "yr \t 67.08548284830512\n",
      "hr \t 144.23724453828655\n",
      "workingday \t 36.04793423065309\n",
      "temp \t 169.99315633922674\n",
      "hum \t -94.82501332221992\n"
     ]
    }
   ],
   "source": [
    "selected_cols = ['yr', 'hr', 'workingday', 'temp', 'hum']\n",
    "x3 = bikes[selected_cols].values\n",
    "x3_train, x3_test = x3[train,:], x3[~train,:]\n",
    "x3_train = min_max_scaler.fit_transform(x3_train)\n",
    "x3_test = min_max_scaler.transform(x3_test)\n",
    "lasso = linear_model.Lasso(alpha = 1.0)\n",
    "lasso.fit(x3_train, y_train)\n",
    "print('alpha:', 1.0)\n",
    "print('R2 score:', lasso.score(x3_test, y_test))\n",
    "for i in range(len(selected_cols)):\n",
    "    print(selected_cols[i], '\\t', lasso.coef_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now think if there is a better way to use column 'dteday' since dates are not numerical values. We convert it to numerical values using data structure datetime with functions strptime and strftime.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.0\n",
      "R2 score: 0.3281515370153293\n",
      "days \t 0 0.0\n",
      "yr \t 1 67.17666255382841\n",
      "mnth \t 2 41.371053791371956\n",
      "hr \t 3 143.46922827639622\n",
      "holiday \t 4 -0.0\n",
      "workingday \t 5 36.7701643851936\n",
      "atemp \t 6 179.11014616281398\n",
      "hum \t 7 -110.09271340534262\n",
      "windspeed \t 8 0.0\n"
     ]
    }
   ],
   "source": [
    "#days = np.zeros((num_row))\n",
    "#for i in range(num_row):\n",
    " #   days[i] = # using bikes.iloc[i]['dteday']\n",
    "##code\n",
    "num_row = len(bikes)\n",
    "days = np.zeros(num_row)\n",
    "for i in  range(num_row):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    d = \"01\"\n",
    "    m = \"01\"\n",
    "    Current_date = datetime.strptime(bikes[\"dteday\"].iloc[i],date_format)\n",
    "    Current_date_year = Current_date.strftime(\"%Y\")\n",
    "    starting_Date = str(Current_date_year)+\"-\"+m+\"-\"+d\n",
    "    starting_Date_year = datetime.strptime(starting_Date ,date_format)\n",
    "    days[i] = abs(Current_date - starting_Date_year).days +1\n",
    "\n",
    "bikes['days'] = days\n",
    "\n",
    "\n",
    "selected_cols = ['days', 'yr', 'mnth', 'hr', 'holiday', 'workingday', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "x4 = bikes[selected_cols].values\n",
    "x4_train, x4_test = x4[train,:], x4[~train,:]\n",
    "x4_train = min_max_scaler.fit_transform(x4_train)\n",
    "x4_test = min_max_scaler.transform(x4_test)\n",
    "lasso = linear_model.Lasso(alpha = 1.0)\n",
    "lasso.fit(x4_train, y_train)\n",
    "print('alpha:', 1.0)\n",
    "print('R2 score:', lasso.score(x4_test, y_test))\n",
    "for i in range(len(selected_cols)):\n",
    "    print(selected_cols[i], '\\t',i, lasso.coef_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak hour 17 \n"
     ]
    }
   ],
   "source": [
    "peak_hour = pd.DataFrame(bikes)[['hr','cnt']].groupby(\"hr\").sum().sort_values([\"cnt\"], ascending=False).index[0]\n",
    "print (\"peak hour {} \".format(peak_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_diff = np.zeros((num_row, 2))\n",
    "for i in  range(len(days)):\n",
    "    hr_diff[i, 0] = peak_hour - bikes[\"hr\"].iloc[i]\n",
    "    hr_diff[i, 1] = peak_hour - bikes[\"hr\"].iloc[i]\n",
    "\n",
    "\n",
    "bikes[\"hr_diff_x\"] = hr_diff[:,0]\n",
    "bikes[\"hr_diff_y\"] = hr_diff[:,1]\n",
    "\n",
    "\n",
    "#selected_cols = [ 'yr', 'mnth', 'hr', 'holiday', 'workingday', 'temp', 'atemp', 'hum', 'windspeed','hr_diff_x','hr_diff_y']\n",
    "\n",
    "month = pd.get_dummies(bikes['mnth'], prefix = 'mnth=')\n",
    "bikes = pd.concat([bikes,month], axis = 1)\n",
    "#selected_cols = ['yr','hum','holiday','workingday','hr_diff_x','temp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.645\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.645\n",
      "Method:                 Least Squares   F-statistic:                              2630.\n",
      "Date:                Wed, 16 Jun 2021   Prob (F-statistic):                        0.00\n",
      "Time:                        23:45:00   Log-Likelihood:                         -54513.\n",
      "No. Observations:                8687   AIC:                                  1.090e+05\n",
      "Df Residuals:                    8681   BIC:                                  1.091e+05\n",
      "Df Model:                           6                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            84.7869      2.687     31.552      0.000      79.519      90.054\n",
      "x2           -14.3741      5.801     -2.478      0.013     -25.745      -3.004\n",
      "x3            56.7839      2.887     19.671      0.000      51.125      62.443\n",
      "x4          -125.1564      4.697    -26.647      0.000    -134.363    -115.949\n",
      "x5           275.6744      5.892     46.786      0.000     264.124     287.225\n",
      "x6             0.8315      3.212      0.259      0.796      -5.465       7.128\n",
      "==============================================================================\n",
      "Omnibus:                     1895.972   Durbin-Watson:                   1.129\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4200.976\n",
      "Skew:                           1.250   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.313   Cond. No.                         7.28\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#selected_cols = ['yr','hum','mnth=_1','mnth=_2','mnth=_3','mnth=_4','mnth=_5','mnth=_6','mnth=_7','mnth=_8','mnth=_9','mnth=_10','mnth=_11','mnth=_12','workingday','hr_diff_x','temp']\n",
    "selected_cols = ['yr','hum','workingday','hr_diff_x','temp','season_=_2']\n",
    "x5 = bikes[selected_cols].values\n",
    "x5_train, x5_test = x5[train,:], x5[~train,:]\n",
    "x5_train = min_max_scaler.fit_transform(x5_train)\n",
    "x5_test = min_max_scaler.transform(x5_test)\n",
    "\n",
    "\n",
    "\"\"\"from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(max_iter=7000 ,alpha = 0.0000000008, tol=1e-3,penalty=\"l2\")\n",
    "reg.fit(x5_train, y_train)\n",
    "score = reg.score(x5_train, y_train)\n",
    "print(\"R-squared  -  Test :\", score)\n",
    "print(\"R-squared  -  predict  :\", reg.score(x5_test, y_test))\"\"\"\n",
    "\n",
    "'''from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "degree=2\n",
    "polyreg=make_pipeline(PolynomialFeatures(degree),linear_model.Lasso(alpha = 0.1))\n",
    "polyreg.fit(x5_train, y_train)\n",
    "score = polyreg.score(x5_train, y_train)\n",
    "print(\"R-squared:\", score)\n",
    "'''\n",
    "\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "poly_x_inliers = poly_reg.fit_transform(x5_train)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(poly_x_inliers, y_train)\n",
    "'''\n",
    "''\n",
    "\"\"\"reg1 = linear_model.Lasso(alpha = 1)\n",
    "reg1.fit(x5_train, y_train)\n",
    "print(\" R-squared_Lesso Model - train:  {} \".format(reg1.score(x5_train, y_train)))\n",
    "print(\" R-squared_Lesso Model - test :  {} \".format(reg1.score(x5_test, y_test)))\n",
    "print(pd.DataFrame(reg1.coef_))\"\"\"\n",
    "\n",
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y_train,x5_train)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "\n",
    "# build x5 as the cell above\n",
    "# build a regression model with x5_train after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [8.09713982e-01 1.56520258e-01 3.19145699e-02 1.73421337e-03\n",
      " 5.99167864e-05 1.20872394e-05 7.36734144e-06]\n",
      "\n",
      " Total Variance Explained: 100.0\n",
      "season             0.870376\n",
      "yr                 0.109938\n",
      "mnth               0.097808\n",
      "hr                 0.609286\n",
      "holiday            0.025316\n",
      "weekday            1.000197\n",
      "workingday         0.111973\n",
      "weathersit         0.709053\n",
      "temp               0.111738\n",
      "atemp              0.101337\n",
      "hum                0.113738\n",
      "windspeed          0.020780\n",
      "casual             0.973238\n",
      "registered         1.190565\n",
      "cnt                1.124979\n",
      "season_=_1         0.393278\n",
      "season_=_2         0.111147\n",
      "season_=_3         0.215547\n",
      "season_=_4         0.171389\n",
      "weekday_=_0        0.155906\n",
      "weekday_=_1        0.075948\n",
      "weekday_=_2        0.072665\n",
      "weekday_=_3        0.030821\n",
      "weekday_=_4        0.066434\n",
      "weekday_=_5        0.084940\n",
      "weekday_=_6        0.147695\n",
      "weathersit_=_1     0.509485\n",
      "weathersit_>=_2    0.509485\n",
      "weathersit_>=_3    0.199082\n",
      "days               1.088145\n",
      "hr_diff_x          0.609286\n",
      "hr_diff_y          0.609286\n",
      "mnth=_1            0.040084\n",
      "mnth=_2            0.080869\n",
      "mnth=_3            0.062272\n",
      "mnth=_4            0.069180\n",
      "mnth=_5            0.028688\n",
      "mnth=_6            0.035324\n",
      "mnth=_7            0.123300\n",
      "mnth=_8            0.058709\n",
      "mnth=_9            0.037400\n",
      "mnth=_10           0.109915\n",
      "mnth=_11           0.054113\n",
      "mnth=_12           0.229847\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 7)\n",
    "x_train3 = pca.fit_transform(bikes.drop(columns = {'dteday','instant'}))\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "loading = pca.components_.T\n",
    "df_loadings  = pd.DataFrame(loading,columns = ['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7'], index = bikes.drop(columns = {'dteday','instant'}).columns).abs().sum(axis=1)\n",
    "print (df_loadings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17379, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "bikes = pd.read_csv('bike_sharing.csv')\n",
    "num_row = bikes.shape[0]\n",
    "y = bikes['registered'].values\n",
    "y = np.array([1 if i <= 60 else 2 if i <= 180 else 3 for i in y])\n",
    "\n",
    "np.random.seed(2021)\n",
    "selection = np.random.choice(['train', 'test', 'rest'], num_row, replace = True, p = [0.01, 0.09, 0.9])\n",
    "y_train, y_test = y[selection == 'train'], y[selection == 'test']\n",
    "bikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17379, 46)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dummy variables creation \n",
    "\n",
    "cols = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "yr = pd.get_dummies(bikes['yr'], prefix = 'yr=')\n",
    "mnth = pd.get_dummies(bikes['mnth'], prefix = 'mnth=')\n",
    "hr = pd.get_dummies(bikes['hr'], prefix = 'hr=')\n",
    "holiday = pd.get_dummies(bikes['holiday'], prefix = 'holiday=')\n",
    "workingday = pd.get_dummies(bikes['workingday'], prefix = 'workingday=')\n",
    "df_ = pd.concat([yr,mnth,hr,holiday,workingday,bikes[cols]], axis = 1)\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape (171, 46), y_train shape : (171,)\n",
      "x_test shape (1551, 46), y_test shape : (1551,)\n"
     ]
    }
   ],
   "source": [
    "#selected_cols = ['yr', 'mnth', 'hr', 'holiday', 'workingday', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "'''Split Data : Train - Test '''\n",
    "\n",
    "x = df_.values\n",
    "x_train, x_test = x[selection == 'train',:], x[selection == 'test',:]\n",
    "\n",
    "## Data Scaling \n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x_train)\n",
    "x_test = min_max_scaler.transform(x_test)\n",
    "print (\"x_train shape {}, y_train shape : {}\".format(x_train.shape,y_train.shape))\n",
    "print (\"x_test shape {}, y_test shape : {}\".format(x_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " At C : 1 \n",
      " R2 Test : 0.6718 , R2-Train : 0.8538 , log_loss_train : 0.59 , log_loss_test : 0.75 \n",
      "\n",
      "Confusion Matrix : \n",
      "                     Predicted Class 1  Predicted Class 2  Predicted Class 3\n",
      "Predicted Class 1                418                 80                 17\n",
      "Predicted Class 2                116                249                147\n",
      "Predicted Class 3                 21                128                375\n",
      "\n",
      " Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.81      0.78       515\n",
      "           2       0.54      0.49      0.51       512\n",
      "           3       0.70      0.72      0.71       524\n",
      "\n",
      "    accuracy                           0.67      1551\n",
      "   macro avg       0.66      0.67      0.67      1551\n",
      "weighted avg       0.66      0.67      0.67      1551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "num_C = 1\n",
    "\n",
    "logit = [None] * num_C\n",
    "inv_log_likelihood_train = [0.0] * num_C\n",
    "inv_log_likelihood_test = [0.0] * num_C\n",
    "\n",
    "R2_Test = []\n",
    "R2_Train = []\n",
    "inv_log_likelihood_train = []\n",
    "inv_log_likelihood_test = []\n",
    "\n",
    "for i in range(1,num_C+1):\n",
    "    #logicReg = LogisticRegression(C=i,random_state=12,solver ='newton-cg', multi_class = 'multinomial')\n",
    "    logicReg = LogisticRegression(C=i, random_state=12,solver ='newton-cg', multi_class = 'ovr')\n",
    "    logicReg.fit(x_train, y_train)\n",
    "    \n",
    "    R2Train = logicReg.score(x_train, y_train)\n",
    "    R2Test = logicReg.score(x_test, y_test)\n",
    "    \n",
    "    R2_Test.append(R2Test)\n",
    "    R2_Train.append(R2Train)\n",
    "    train_predict_probbility = logicReg.predict_proba(x_train)\n",
    "    test_predict_probbility = logicReg.predict_proba(x_test)  \n",
    "    \n",
    "    log_loss_train = log_loss(y_train, train_predict_probbility,eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "    log_loss_test = log_loss(y_test, test_predict_probbility,eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "    \n",
    "    inv_log_likelihood_train.append(log_loss_train)\n",
    "    inv_log_likelihood_test.append(log_loss_test)\n",
    "    \n",
    "print (\"\\n At C : {} \\n R2 Test : {:.4f} , R2-Train : {:.4f} , log_loss_train : {:.2f} , log_loss_test : {:.2f} \".format(i,R2Test,R2Train,log_loss_train,log_loss_test))\n",
    "col = [\"Predicted Class 1\",\"Predicted Class 2\",\"Predicted Class 3\"]\n",
    "print(\"\\nConfusion Matrix : \\n  {}\".format(pd.DataFrame(confusion_matrix(y_test,logicReg.predict(x_test)), columns = col, index =col)))\n",
    "\n",
    "print(\"\\n Classification report:\\n {}\".format(classification_report(y_test,logicReg.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At C : 1 ,R2 Test : 0.6809 ,R2-Train : 0.8596 ,inv_log_likelihood_train : 0.51 , inv_log_likelihood_test : 0.72 \n",
      "At C : 2 ,R2 Test : 0.6828 ,R2-Train : 0.8655 ,inv_log_likelihood_train : 0.44 , inv_log_likelihood_test : 0.71 \n",
      "At C : 3 ,R2 Test : 0.6841 ,R2-Train : 0.8713 ,inv_log_likelihood_train : 0.40 , inv_log_likelihood_test : 0.72 \n",
      "At C : 4 ,R2 Test : 0.6841 ,R2-Train : 0.8713 ,inv_log_likelihood_train : 0.38 , inv_log_likelihood_test : 0.74 \n",
      "At C : 5 ,R2 Test : 0.6821 ,R2-Train : 0.8655 ,inv_log_likelihood_train : 0.36 , inv_log_likelihood_test : 0.75 \n",
      "At C : 6 ,R2 Test : 0.6796 ,R2-Train : 0.8713 ,inv_log_likelihood_train : 0.35 , inv_log_likelihood_test : 0.77 \n",
      "At C : 7 ,R2 Test : 0.6809 ,R2-Train : 0.8713 ,inv_log_likelihood_train : 0.34 , inv_log_likelihood_test : 0.79 \n",
      "At C : 8 ,R2 Test : 0.6815 ,R2-Train : 0.8713 ,inv_log_likelihood_train : 0.33 , inv_log_likelihood_test : 0.80 \n",
      "At C : 9 ,R2 Test : 0.6809 ,R2-Train : 0.8713 ,inv_log_likelihood_train : 0.33 , inv_log_likelihood_test : 0.81 \n",
      "At C : 10 ,R2 Test : 0.6821 ,R2-Train : 0.8713 ,inv_log_likelihood_train : 0.32 , inv_log_likelihood_test : 0.83 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "num_C = 10\n",
    "C = [1.0] * num_C\n",
    "for i in range(num_C):\n",
    "    C[i] = pow(10, i-3)\n",
    "\n",
    "logit = [None] * num_C\n",
    "inv_log_likelihood_train = [0.0] * num_C\n",
    "inv_log_likelihood_test = [0.0] * num_C\n",
    "\n",
    "R2_Test = []\n",
    "R2_Train = []\n",
    "inv_log_likelihood_train = []\n",
    "inv_log_likelihood_test = []\n",
    "\n",
    "for i in range(1,num_C+1):\n",
    "    logicReg = LogisticRegression(C=i,random_state=12,solver ='newton-cg', multi_class = 'multinomial')\n",
    "    #logicReg = LogisticRegression(C=i, random_state=12,solver ='newton-cg', multi_class = 'ovr')\n",
    "    logicReg.fit(x_train, y_train)\n",
    "    \n",
    "    R2Train = logicReg.score(x_train, y_train)\n",
    "    R2Test = logicReg.score(x_test, y_test)\n",
    "    \n",
    "    R2_Test.append(R2Test)\n",
    "    R2_Train.append(R2Train)\n",
    "    train_predict_probbility = logicReg.predict_proba(x_train)\n",
    "    test_predict_probbility = logicReg.predict_proba(x_test)  \n",
    "    \n",
    "    log_loss_train = log_loss(y_train, train_predict_probbility,eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "    log_loss_test = log_loss(y_test, test_predict_probbility,eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "    \n",
    "    inv_log_likelihood_train.append(log_loss_train)\n",
    "    inv_log_likelihood_test.append(log_loss_test)\n",
    "    \n",
    "    print (\"At C : {} ,R2 Test : {:.4f} ,R2-Train : {:.4f} ,inv_log_likelihood_train : {:.2f} , inv_log_likelihood_test : {:.2f} \".format(i,R2Test,R2Train,log_loss_train,log_loss_test))\n",
    "\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.xscale('log')\n",
    "plt.plot(C, inv_log_likelihood_train, 'bo-', C, inv_log_likelihood_test, 'ro-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6381d38b9e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classification_report(y_test,predict)\n",
    "cm = confusion_matrix(y_test,predict)\n",
    "predict = lr.predict(x_test)\n",
    "## gives predict probability \n",
    "pd.DataFrame(lr.predict_proba(x_test))\n",
    "\n",
    "## r \n",
    "\n",
    "print('R2 score train:', lr.score(x_train, y_train))\n",
    "lr.fit(x_train, y_train)\n",
    "print('R2 score test :', lr.score(x_test, y_test))\n",
    "\n",
    "## Classification report \n",
    "\n",
    "print(classification_report(y_test,predict))\n",
    "print(confusion_matrix(y_test,predict))\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "clf = SVC(random_state=0)\n",
    "plot_confusion_matrix(lr,x_test, y_test)  \n",
    "plt.show()  \n",
    "#pd.DataFrame(plot_confusion_matrix)\n",
    "             \n",
    "## predict prob\n",
    "print(\"predic proba\")\n",
    "train_predict_probbility = lr.predict_proba(x_train)\n",
    "test_predict_probbility = lr.predict_proba(y_train)\n",
    "#print(lr.predict_proba(x_train))\n",
    "## loss '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Linear Function\\n2. Sigmoid function\\n3. cost function  ( Cross entropy)\\n4. gradient descent - Learning rate \\n\\n5 - https://realpython.com/logistic-regression-python/\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Linear Function\n",
    "2. Sigmoid function\n",
    "3. cost function  ( Cross entropy)\n",
    "4. gradient descent - Learning rate \n",
    "\n",
    "5 - https://realpython.com/logistic-regression-python/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print (\"Log loss on train data :\")\n",
    "log_loss(y_train, train_predict_probbility,eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "print (\"Log loss on test data :\")\n",
    "log_loss(y_test, test_predict_probbility,eps=1e-15, normalize=True, sample_weight=None, labels=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
